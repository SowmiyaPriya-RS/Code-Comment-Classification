{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.46.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n",
      "---\n",
      "Name: datasets\n",
      "Version: 3.1.0\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\n",
      "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, tqdm, xxhash\n",
      "Required-by: \n",
      "---\n",
      "Name: torch\n",
      "Version: 2.5.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: \n",
      "---\n",
      "Name: scikit-learn\n",
      "Version: 1.5.2\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: https://scikit-learn.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: BSD 3-Clause License\n",
      "\n",
      "Copyright (c) 2007-2024 The scikit-learn developers.\n",
      "All rights reserved.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "* Redistributions of source code must retain the above copyright notice, this\n",
      "  list of conditions and the following disclaimer.\n",
      "\n",
      "* Redistributions in binary form must reproduce the above copyright notice,\n",
      "  this list of conditions and the following disclaimer in the documentation\n",
      "  and/or other materials provided with the distribution.\n",
      "\n",
      "* Neither the name of the copyright holder nor the names of its\n",
      "  contributors may be used to endorse or promote products derived from\n",
      "  this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "Location: c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: \n",
      "---\n",
      "Name: pandas\n",
      "Version: 2.2.3\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: https://pandas.pydata.org\n",
      "Author: \n",
      "Author-email: The Pandas Development Team <pandas-dev@python.org>\n",
      "License: BSD 3-Clause License\n",
      "\n",
      "Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\n",
      "All rights reserved.\n",
      "\n",
      "Copyright (c) 2011-2023, Open source contributors.\n",
      "\n",
      "Redistribution and use in source and binary forms, with or without\n",
      "modification, are permitted provided that the following conditions are met:\n",
      "\n",
      "* Redistributions of source code must retain the above copyright notice, this\n",
      "  list of conditions and the following disclaimer.\n",
      "\n",
      "* Redistributions in binary form must reproduce the above copyright notice,\n",
      "  this list of conditions and the following disclaimer in the documentation\n",
      "  and/or other materials provided with the distribution.\n",
      "\n",
      "* Neither the name of the copyright holder nor the names of its\n",
      "  contributors may be used to endorse or promote products derived from\n",
      "  this software without specific prior written permission.\n",
      "\n",
      "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "Location: c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\n",
      "Requires: numpy, python-dateutil, pytz, tzdata\n",
      "Required-by: datasets\n",
      "---\n",
      "Name: tqdm\n",
      "Version: 4.67.1\n",
      "Summary: Fast, Extensible Progress Meter\n",
      "Home-page: https://tqdm.github.io\n",
      "Author: \n",
      "Author-email: \n",
      "License: MPL-2.0 AND MIT\n",
      "Location: c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\n",
      "Requires: colorama\n",
      "Required-by: datasets, huggingface-hub, transformers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers datasets torch scikit-learn pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label dictionaries for each language\n",
    "labels_dict = {\n",
    "    \"java\": [\"summary\", \"Ownership\", \"Expand\", \"usage\", \"Pointer\", \"deprecation\", \"rational\"],\n",
    "    \"python\": [\"Usage\", \"Parameters\", \"DevelopmentNotes\", \"Expand\", \"Summary\"],\n",
    "    \"pharo\": [\"Keyimplementationpoints\", \"Example\", \"Responsibilities\", \"Classreferences\", \"Intent\", \"Keymessages\", \"Collaborators\"]\n",
    "}\n",
    "\n",
    "# Load the dataset from HuggingFace\n",
    "dataset_name = \"NLBSE/nlbse25-code-comment-classification\"\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "def prepare_data_loaders(data_split, tokenizer, max_length=128, batch_size=32):\n",
    "    # Tokenize the data\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"comment_sentence\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    \n",
    "    tokenized_data = data_split.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Ensure labels are correctly formatted (convert labels to Long type)\n",
    "    tokenized_data = tokenized_data.map(lambda x: {'labels': torch.tensor(x['labels'], dtype=torch.float32)}, batched=True)\n",
    "    tokenized_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    return DataLoader(tokenized_data, batch_size=batch_size)\n",
    "\n",
    "# Train a language-specific model\n",
    "def train_language_model(lang, dataset, num_labels, epochs=3, lr=5e-5):\n",
    "    print(f\"Training model for {lang}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "    train_loader = prepare_data_loaders(dataset[\"train\"], tokenizer)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Loss Function for Multi-Label Classification\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Training loop\n",
    "    total_flops = 0\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device).float()  # Labels should be Float type now\n",
    "            \n",
    "            if batch_idx == 0:  # Skip first batch to warm up GPU\n",
    "                continue\n",
    "\n",
    "            with torch.profiler.profile(\n",
    "                activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "                with_flops=True\n",
    "            ) as p:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "            \n",
    "                # Calculate the loss for multi-label classification\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Summing FLOPs for this batch (convert to GFLOPs)\n",
    "            total_flops += sum(k.flops for k in p.key_averages()) / 1e12\n",
    "\n",
    "    return model, total_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing java...\n",
      "Training model for java...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/238 [00:00<?, ?it/s]c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\\torch\\autograd\\profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n",
      "100%|██████████| 238/238 [32:08<00:00,  8.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [32:30<00:00,  8.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [37:36<00:00,  9.48s/it]\n",
      "c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language     Category  Precision    Recall        F1\n",
      "0     java      summary   0.786517  0.863229  0.823089\n",
      "1     java    Ownership   0.978261  1.000000  0.989011\n",
      "2     java       Expand   0.205128  0.078431  0.113475\n",
      "3     java        usage   0.960656  0.679814  0.796196\n",
      "4     java      Pointer   0.737991  0.918478  0.818402\n",
      "5     java  deprecation   0.000000  0.000000  0.000000\n",
      "6     java     rational   0.100840  0.176471  0.128342\n",
      "Processing python...\n",
      "Training model for python...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59 [00:00<?, ?it/s]c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\\torch\\autograd\\profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n",
      "100%|██████████| 59/59 [06:33<00:00,  6.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [06:17<00:00,  6.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [06:16<00:00,  6.38s/it]\n",
      "c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language          Category  Precision    Recall        F1\n",
      "0   python             Usage   0.758621  0.545455  0.634615\n",
      "1   python        Parameters   0.632653  0.484375  0.548673\n",
      "2   python  DevelopmentNotes   0.000000  0.000000  0.000000\n",
      "3   python            Expand   0.318182  0.109375  0.162791\n",
      "4   python           Summary   0.563107  0.707317  0.627027\n",
      "Processing pharo...\n",
      "Training model for pharo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:00<?, ?it/s]c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\\torch\\autograd\\profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n",
      "100%|██████████| 41/41 [04:28<00:00,  6.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [04:10<00:00,  6.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [04:07<00:00,  6.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language                 Category  Precision    Recall        F1\n",
      "0    pharo  Keyimplementationpoints   0.000000  0.000000  0.000000\n",
      "1    pharo                  Example   0.904762  0.638655  0.748768\n",
      "2    pharo         Responsibilities   0.487805  0.769231  0.597015\n",
      "3    pharo          Classreferences   0.000000  0.000000  0.000000\n",
      "4    pharo                   Intent   0.870968  0.900000  0.885246\n",
      "5    pharo              Keymessages   0.000000  0.000000  0.000000\n",
      "6    pharo            Collaborators   0.000000  0.000000  0.000000\n",
      "Average F1:  0.4143500123270708\n",
      "Average Runtime:  7.4562011747448524\n",
      "Average Flops:  239.18828599705384\n",
      "Compute in GFLOPs: 239.18828599705384\n",
      "Avg runtime in seconds: 7.4562011747448524\n",
      "Final Score: 0.34\n",
      "   Language                 Category  Precision    Recall        F1\n",
      "0      java                  summary   0.786517  0.863229  0.823089\n",
      "1      java                Ownership   0.978261  1.000000  0.989011\n",
      "2      java                   Expand   0.205128  0.078431  0.113475\n",
      "3      java                    usage   0.960656  0.679814  0.796196\n",
      "4      java                  Pointer   0.737991  0.918478  0.818402\n",
      "5      java              deprecation   0.000000  0.000000  0.000000\n",
      "6      java                 rational   0.100840  0.176471  0.128342\n",
      "7    python                    Usage   0.758621  0.545455  0.634615\n",
      "8    python               Parameters   0.632653  0.484375  0.548673\n",
      "9    python         DevelopmentNotes   0.000000  0.000000  0.000000\n",
      "10   python                   Expand   0.318182  0.109375  0.162791\n",
      "11   python                  Summary   0.563107  0.707317  0.627027\n",
      "12    pharo  Keyimplementationpoints   0.000000  0.000000  0.000000\n",
      "13    pharo                  Example   0.904762  0.638655  0.748768\n",
      "14    pharo         Responsibilities   0.487805  0.769231  0.597015\n",
      "15    pharo          Classreferences   0.000000  0.000000  0.000000\n",
      "16    pharo                   Intent   0.870968  0.900000  0.885246\n",
      "17    pharo              Keymessages   0.000000  0.000000  0.000000\n",
      "18    pharo            Collaborators   0.000000  0.000000  0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\senth\\Desktop\\code-comment-classification\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate metrics (precision, recall, f1 score)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Evaluate the model and calculate metrics\n",
    "def evaluate_model(lang, dataset, model):\n",
    "    model.eval()\n",
    "    test_loader = prepare_data_loaders(dataset[\"test\"], AutoTokenizer.from_pretrained(\"bert-base-uncased\"))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = torch.sigmoid(model(**inputs).logits)\n",
    "            preds = (outputs > 0.4).int()\n",
    "\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Calculate per-label precision, recall, and F1 score\n",
    "    label_list = labels_dict[lang]\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
    "    \n",
    "    # Create a dataframe for better display\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"Language\": [lang] * len(label_list),\n",
    "        \"Category\": label_list,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1\n",
    "    })\n",
    "\n",
    "    print(metrics_df)\n",
    "    return metrics_df\n",
    "\n",
    "def score(avg_f1, avg_runtime, avg_flops, max_avg_runtime=5, max_avg_flops=5000):\n",
    "    return (0.6 * avg_f1 +\n",
    "            0.2 * ((max_avg_runtime - avg_runtime) / max_avg_runtime) +\n",
    "            0.2 * ((max_avg_flops - avg_flops) / max_avg_flops))\n",
    "\n",
    "# Main workflow\n",
    "results = []\n",
    "total_time = 0\n",
    "total_flops = 0\n",
    "\n",
    "for lang in labels_dict.keys():\n",
    "    print(f\"Processing {lang}...\")\n",
    "    num_labels = len(labels_dict[lang])\n",
    "\n",
    "    # Filter the dataset for the specific language\n",
    "    language_dataset = {\n",
    "        \"train\": dataset[f\"{lang}_train\"].map(lambda x: {\"labels\": torch.tensor(x[\"labels\"], dtype=torch.float32)}),\n",
    "        \"test\": dataset[f\"{lang}_test\"].map(lambda x: {\"labels\": torch.tensor(x[\"labels\"], dtype=torch.float32)})\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Train and evaluate\n",
    "    model, flops = train_language_model(lang, language_dataset, num_labels)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    total_time += (elapsed_time/360)\n",
    "    total_flops += (flops/3)\n",
    "\n",
    "    lang_metrics = evaluate_model(lang, language_dataset, model)\n",
    "    results.append(lang_metrics)\n",
    "\n",
    "avg_f1 = pd.concat(results).F1.mean()\n",
    "avg_runtime = total_time / (len(labels_dict))\n",
    "avg_flops = total_flops / (len(labels_dict))\n",
    "\n",
    "norm_avg_flops = avg_flops\n",
    "norm_avg_runtime = avg_runtime\n",
    "\n",
    "print(\"Average F1: \", avg_f1)\n",
    "print(\"Average Runtime: \", norm_avg_runtime)\n",
    "print(\"Average Flops: \", norm_avg_flops)\n",
    "\n",
    "final_score = round(score(avg_f1, norm_avg_runtime, norm_avg_flops), 2)\n",
    "print(f\"Compute in GFLOPs: {norm_avg_flops}\")\n",
    "print(f\"Avg runtime in seconds: {norm_avg_runtime}\")\n",
    "print(f\"Final Score: {final_score}\")\n",
    "\n",
    "# Combine results into a single dataframe\n",
    "final_results = pd.concat(results, ignore_index=True)\n",
    "print(final_results)\n",
    "\n",
    "# Save the results to a CSV file if needed\n",
    "final_results.to_csv(\"classification_metrics.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
